# Exploratory Data Analysis (EDA) 5 {#eda5}

### Course Contents {-}

  1. 2021-12-08: Introduction: About the course  
    - An introduction to open and public data, and data science
  2. 2021-12-15: Exploratory Data Analysis (EDA) 1 [lead by hs]  
    - R Basics with RStudio and/or RStudio.cloud; R Script, swirl
  3. 2021-12-22: Exploratory Data Analysis (EDA) 2 [lead by hs]   
    - R Markdown; Introduction to `tidyverse`; RStudio Primers
  4. 2022-01-12: Exploratory Data Analysis (EDA) 3 [lead by hs]  
    - Introduction to `tidyverse`; Public Data, WDI, etc
  5. 2022-01-19: Exploratory Data Analysis (EDA) 4 [lead by hs]  
    - Introduction to `tidyverse`; WDI, UN, WHO, etc
  6. **2022-01-26: Exploratory Data Analysis (EDA) 5** [lead by hs]  
    - **Introduction to `tidyverse`; UN, WHO,OECD, US gov, etc**
  7. 2022-02-02: Inference Statistics 1
  8. 2022-02-09: Inference Statistics 2
  9. 2022-02-16: Inference Statistics 3
  10. 2022-02-23: Project Presentation
  
### Data Modeling and EDA {-}

* Modeling using `tidyverse`

#### EDA (A diagram from R4DS by H.W. and G.G.) {-}

![EDA from r4ds](./data/data-science.png)

### Contents of EDA5 {-}

> Data Science is empirical!?

**empirical**: based on, concerned with, or verifiable by observation or experience rather than theory or pure logic  

**Part I: Data Modeling**

_Introduction to Modeling and EDA, in Moodle_

* Exploration in Visualization

* Modeling

  -  Scientific: Why? Prediction! - Do you support this?
  -  Evidence based! - What does it mean? 
  -  What is regression, and why regression? 
  -  Linear Regression, ggplot2
  -  Predictions and Residues

**Part II: Examples using Public Data and Roundup**




## Part I: Exploratory Data Analysis and Data Modeling


### Exploration in Visualization, I -- [r4ds:EDA](https://r4ds.had.co.nz/exploratory-data-analysis.html#exploratory-data-analysis)

EDA is an iterative cycle. You:

1. Generate questions about your data.

2. Search for answers by visualising, transforming, and modelling your data.

3. Use what you learn to refine your questions and/or generate new questions.

\vfill
> “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox (1924-2022)

\vfill
> “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey (1915-2000)

### Exploration in Visualization, II -- [r4ds:EDA](https://r4ds.had.co.nz/exploratory-data-analysis.html#exploratory-data-analysis)

**The very basic questions:**

* What type of variation occurs within my variables?

* What type of covariation occurs between my variables?

\vfill
**Typical values**

* Which values are the most common? Why?

* Which values are rare? Why? Does that match your expectations?

* Can you see any unusual patterns? What might explain them?

### Exploration in Visualization, III -- [r4ds:EDA](https://r4ds.had.co.nz/exploratory-data-analysis.html#exploratory-data-analysis)

**Clusters and Groups**

* How are the observations within each cluster similar to each other?

* How are the observations in separate clusters different from each other?

* How can you explain or describe the clusters?

* Why might the appearance of clusters be misleading?  

\vfill
**Outliers and Unusual Values**

Sometimes outliers are data entry errors; other times outliers suggest important new science. 

### Exploration in Visualization, IV -- [r4ds:EDA](https://r4ds.had.co.nz/exploratory-data-analysis.html#exploratory-data-analysis)

**Patterns and models** 

* Could this pattern be due to coincidence (i.e. random chance)?

* How can you describe the relationship implied by the pattern?

* How strong is the relationship implied by the pattern?

* What other variables might affect the relationship?

* Does the relationship change if you look at individual subgroups of the data?

  

### EDA and Data Modeling: Simple Summary of Data

> **Goal**: A simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in). 

  1. “predictive” models: supervised
  2. “data discovery” models: unsupervised
	
#### EDA (A diagram from R4DS by H.W. and G.G.)

![EDA from r4ds](./data/data-science.png)

### Hypothesis generation vs. hypothesis confirmation

1. Each observation can either be used for exploration or confirmation, not both.

2. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration.

If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis:

1. 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it.

2. 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process.

3. 20% is held back for a test set. You can only use this data ONCE, to test your final model.

### [R4DS: Model Basics](https://r4ds.had.co.nz/model-basics.html#model-basics)

There are two parts to a model:

1. First, you define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like $y = a_1 * x + a_2$ or $y = a_1 * x ^ {a_2}$. Here, $x$ and $y$ are known variables from your data, and $a_1$ and $a_2$ are parameters that can vary to capture different patterns.

2. Next, you generate a fitted model by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like $y = 3 * x + 7$ or $y = 9 * x ^ 2$.

It’s important to understand that **a fitted model is just the closest model from a family of models**. That implies that you have the “best” model (according to some criteria); it doesn’t imply that you have a good model and it certainly doesn’t imply that the model is “true”. 

### All models are wrong, but some are useful.   \hfill - George E.P. Box (1919-2013)

Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an “ideal” gas via a constant R is not exactly true for any real gas, but it frequently provides a **useful approximation** and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.

For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “**Is the model illuminating and useful?**”.

\medskip
> **The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.**

### Regression Analysis ([wikipedia](https://en.wikipedia.org/wiki/Regression_analysis))

In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable ('outcome variable') and one or more independent variables ('predictors', 'covariates', or 'features'). The most common form of regression analysis is linear regression, in which a researcher finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. 

Two purposes of regression analysis: First, regression analysis is widely used for **prediction and forecasting**. Second, regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. 

### History of Regression Analysis  ([wikipedia](https://en.wikipedia.org/wiki/Regression_analysis))


The earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss–Markov theorem.

The term "regression" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to **regress** down towards a normal average (a phenomenon also known as regression toward the mean). For Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context.

### The First Example

```{r}
plot(cars)
```

---
```{r}
plot(cars) # cars: Speed and Stopping Distances of Cars
abline(lm(cars$dist~cars$speed))
```

---
```{r}
summary(lm(cars$dist~cars$speed))
```

### Model Analysis with `tidyverse`: `modelr`

* Tidymodels: https://www.tidymodels.org
* `modelr`: https://modelr.tidyverse.org
  - [Compute model quality for a given dataset](https://modelr.tidyverse.org/reference/model-quality.html)

```{r}
library(tidyverse)
library(modelr)
t_cars <- as_tibble(cars)
(mod <- lm(dist ~ speed, data = t_cars))
```
---
```{r echo = FALSE}
summary(mod)
```

---
```{r}
ggplot(t_cars, aes(speed, dist)) + geom_point() +
  geom_abline(slope = coef(mod)[[2]], intercept = coef(mod)[[1]])
```

---
```{r message = FALSE}
ggplot(t_cars, aes(speed, dist)) + geom_jitter() +
  geom_smooth(method = "lm", se = FALSE)
```

### Predictions $\hat{y} = a_1 + a_2x$ and Residuals $y - \hat{y}$

```{r}
(mod_table <- t_cars %>% add_predictions(mod) %>% add_residuals(mod, var = "resid"))
```

---
```{r}
mod_table %>% ggplot() + geom_jitter(aes(speed, resid)) + geom_hline(yintercept = 0)
```
### geom_jitter()

```{r}
t_cars %>% group_by(speed, dist) %>% 
  summarize(count = n()) %>% arrange(desc(count))
```


### iris

```{r}
(t_iris <- as_tibble(iris))
```

### Linear Model: Sepal.W ~ Sepal.L

```{r}
colnames(t_iris) <- c("Sepal.L", "Sepal.W", "Petal.L", "Petal.W", "Species")
```

```{r}
lm(Sepal.W ~ Sepal.L, data = t_iris)
```

### Linear Model: Petal.W ~ Petal.L

```{r}
lm(Petal.W ~ Petal.L, data = t_iris)
```

### Correlation

```{r}
cor(t_iris[1:4])
```

### `filter(Species == "setosa")`
```
t_iris %>% filter(Species == "setosa") %>%
  ggplot(aes(x = Petal.L, y = Petal.W)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) 
```
---

```{r echo = FALSE, message = FALSE}
t_iris %>% filter(Species == "setosa") %>%
  ggplot(aes(x = Petal.L, y = Petal.W)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Setosa Only")
```

### `aes(Sepal.L, Sepal.W, color = Species)`

```{r message = FALSE, echo = FALSE}
ggplot(t_iris, aes(Sepal.L, Sepal.W, color = Species)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) 
```

### Linear Regression Quick Reference

* DataCamp: 
  - https://www.datacamp.com/community/tutorials/linear-regression-R
    1. What is a linear regression?
    2.  Creating a linear regression in R.
    3. Learn the concepts of coefficients and residuals.
    4. How to test if your linear model has a good fit?
    5. Detecting influential points.
  - DataCamp Top: https://www.datacamp.com - to be introduced

* r-statistics.co by Selva Prabhakaran: 
  - http://r-statistics.co/Linear-Regression.html
    + The aim of linear regression is to model a continuous variable Y as a mathematical function of one or more X variable(s), so that we can use this regression model to predict the Y when only the X is known. This mathematical equation can be generalized as follows:
    + $Y = \beta_1 + \beta_2 X + \varepsilon$
  - r-statistics.co Top: http://r-statistics.co
    + An educational resource for those seeking knowledge related to machine learning and statistical computing in R.


## Part II: Practicum - Model Analysis Warnings

### Linear Regression: `aes(Sepal.L, Sepal.W, color = Species)`

```{r message = FALSE, echo = FALSE}
ggplot(t_iris, aes(Sepal.L, Sepal.W, color = Species)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) 
```

### `aes(Sepal.L, Sepal.W, color = Species)`

```{r message = FALSE, echo = FALSE}
ggplot(t_iris, aes(x = Sepal.L, y = Sepal.W)) + 
  geom_point(aes(color = Species)) + 
  geom_smooth(aes(color = Species), formula = y ~ x, method = "lm", se = FALSE) +
  geom_smooth(formula =  y ~ x, method = "lm", se = FALSE, color = "black", linetype = "twodash", size = 1.5)
```

### Correlation and Covariance

```{r}
cor(t_iris[1:4])
```

```{r}
cov(t_iris[1:4])
```

### Useful Mathematical Formula

* Let $x = c(x_1, x_2, \ldots, x_n)$ be the independent variable, i.e., Sepal.L
* Let $y = c(y_1, y_2, \ldots, y_n)$ be the dependent variable, i.e., Sepal.W
* Let $\mbox{pred} = c(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_n)$ be the predicted values by linear regression.
$$
\begin{aligned}
\mbox{slope of the regression line}  &= \frac{cov(x,y)}{var(x)} = \frac{cor(x,y)\sqrt{var(y)}}{\sqrt{var(x)}}\\
\mbox{total sum of squares} &= SS_{tot} = sum((y-mean(y))^2)\\
\mbox{residual sum of squares} &= SS_{res} = sum((y-\mbox{pred})^2)\\
\mbox{R squared} = R^2 & = 1 - \frac{SS_{res}}{SS_{tot}} = cor(x,y)^2
\end{aligned}
$$



<!-- ### Another Example -->

<!-- ```{r echo = FALSE, message = FALSE} -->
<!-- cancer_death_rate_by_age <- read_csv("_extra_old/assignment6/cancer-death-rates-by-age.csv") -->
<!-- cancer_death_rate_wide <- cancer_death_rate_by_age %>%  -->
<!--   select(Entity:Year,  -->
<!--          all_ages = `Deaths - Neoplasms - Sex: Both - Age: All Ages (Rate)`, -->
<!--          under_5 = `Deaths - Neoplasms - Sex: Both - Age: Under 5 (Rate)`, -->
<!--          `5-14` = `Deaths - Neoplasms - Sex: Both - Age: 5-14 years (Rate)`, -->
<!--          `15-49` = `Deaths - Neoplasms - Sex: Both - Age: 15-49 years (Rate)`, -->
<!--          `50-69` = `Deaths - Neoplasms - Sex: Both - Age: 50-69 years (Rate)`, -->
<!--          over_70 = `Deaths - Neoplasms - Sex: Both - Age: Age-standardized (Rate)`) -->
<!-- cancer_death_rate <- cancer_death_rate_wide %>% -->
<!--   pivot_longer(cols = all_ages:over_70, names_to = "Age_Group", values_to = "Rate") %>% -->
<!--   mutate(Age_Group = fct_inorder(Age_Group)) -->
<!-- Country <- "Sri Lanka" -->
<!-- srilanka <- cancer_death_rate %>% filter(Entity == Country)  -->
<!-- srilanka %>%  ggplot(mapping = aes(x = Year, y = Rate)) +  -->
<!--   geom_point(aes(color = Age_Group)) + -->
<!--   geom_smooth(aes(linetype = Age_Group), formula = y ~ x, method = "lm", se = FALSE, color = "black") + -->
<!--   labs(title = "Rate of Death by Cancer in Ages Groups of a Country") -->
<!-- ``` -->

<!-- ### Is this OK? -->

<!-- ```{r echo = FALSE, message = FALSE} -->
<!-- srilanka %>% -->
<!--   ggplot(mapping = aes(x = Year, y = Rate)) + -->
<!--   geom_point() + -->
<!--   geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + -->
<!--   labs(title = "Rate of Death by Cancer of a Country") -->
<!-- ``` -->


<!-- ### Linear Model Behind -->

<!-- ```{r echo = FALSE} -->
<!-- Country <- "Sri Lanka" -->
<!-- cancer_death_rate %>% filter(Entity == Country) %>% -->
<!--   lm(Rate ~ Year, .) %>% summary() -->
<!-- ``` -->


<!-- ### One Age Group: 50-69 (R-squared:  0.3101) -->

<!-- ```{r echo = FALSE} -->
<!-- srilanka %>% filter(Entity == Country & Age_Group == "50-69") %>% -->
<!--   lm(Rate ~ Year, .) %>% summary() -->
<!-- ``` -->


<!-- ### One Age Group: 50-69 -->

<!-- ```{r echo = FALSE} -->
<!-- Country <- "Sri Lanka" -->
<!-- cancer_death_rate %>% filter(Entity == Country & Age_Group == "50-69") %>% -->
<!--   ggplot(aes(x = Year, y = Rate)) + geom_point() +  -->
<!--   geom_smooth(formula = y~x, method = "lm", se = FALSE) -->
<!-- ``` -->


<!-- ### One Age Group: 50-69 -->

<!-- ```{r echo = FALSE} -->
<!-- srilanka %>% filter(Entity == Country & Age_Group == "50-69") %>% -->
<!--   ggplot(aes(x = Year, y = Rate)) + geom_point() +  -->
<!--   geom_smooth(formula = y~x, method = "lm", se = FALSE) +  -->
<!--   scale_y_continuous(limits = c(0, NA)) -->
<!-- ``` -->

### p-value

```{r, echo = FALSE}
ggplot(data = data.frame(x = c(-1.5, 0.5)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = -0.7907, sd = 0.2313)) + 
  geom_vline(xintercept = 0, color = "red", linetype = "dotted", size = 1.5) +
  labs(title = "Slope of Rate ~ Year Predition", 
       subtitle = "slope = -0.7907, se = 0.2313, t = -3.418, p-value = 0.002086, df = 26")
```

### t-statistic

```{r, echo = FALSE}
df <- 48 # degree of freedom
funcShaded <- function(x) {
    y <- dt(x, df)
    y[x < -1.96 | x > 1.96] <- NA
    return(y)
}
funcShaded2 <- function(x) {
    y <- dt(x, df)
    y[x > -1.92 & x < 1.92] <- NA
    return(y)
}
ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = function(x){dt(x, df)}, color = "black", size = 1) + 
  stat_function(fun = funcShaded, geom = "area", fill = "blue", alpha = 0.5) +
  stat_function(fun = funcShaded2, geom = "area", fill = "red", alpha = 0.5) +
  geom_vline(xintercept = -2.601, color = "black", linetype = "dotted", size = 1.5) + 
  labs(title = "95% Interval: p(-1.96 < x < 1.96)",
       subtitle = "The t value of the y-intercept estimate is -2.601, and its p-value Pr(>|t|) is 0.0123") +
  annotate("text", label = "0.95", x = 0, y = 0.2, size = 6) +
  annotate("text", label = "95%", x = 0, y = 0.1, size = 8) +
  annotate("text", label = "0.025", x = 2.7, y = 0.06, size = 6) +
  annotate("text", label = "0.025", x = -2.7, y = 0.06, size = 6)
```

### Mathematical Formulas, I

* Let $x = c(x_1, x_2, \ldots, x_n)$ be the independent variable, e.g., the height of fathers
* Let $y = c(y_1, y_2, \ldots, y_n)$ be the dependent variable, i.e., height of children
* Let $\mbox{pred} = c(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_n)$ be the predicted values by linear regression.

### Mathematical Formulas, II

$$
\begin{aligned}
\mbox{unbiased covariance: } cov(x,y) &= \frac{\sum_{i=1}^n(x_i - mean(x))(y_i - mean(y))}{n-1}\\
\mbox{unviased variance: } var(x) &= cov(x,x) \\
\mbox{correlation: } cor(x.y) &= \frac{cov(x,y)}{\sqrt{var(x)var(y)}}\\
\mbox{slope of the regression line}  &= \frac{cov(x,y)}{var(x)} = \frac{cor(x,y)\sqrt{var(y)}}{\sqrt{var(x)}}\\
\mbox{total sum of squares} &= SS_{tot} = \sum_{i=1}^n((y_i-mean(y))^2)\\
\mbox{residual sum of squares} &= SS_{res} = \sum_{i=1}^n((y_i-\hat{y}_i)^2)\\
\mbox{R squared} = R^2 & = 1 - \frac{SS_{res}}{SS_{tot}} = cor(x,y)^2
\end{aligned}
$$

### Regression Analyais: Summary

1. R-Squared:	Higher the better (> 0.70)
   - How well the prediction fit to the data. $0 \leq R2 \leq 1$.
   - For linear regression between two variables `x` and `y`, R-squared is the square of `cor(x,y)`.
   - R-Squared can be measured on any prediction model.
2. t-statistic:	The absolute value should be greater 1.96, and for p-value be less than 0.05
   - The model and the choice of variable(s) are suitable or not.
   - p-value appears in Hypothesis testing (A/B test, sensitivity - specificity, etc.)

   
### [The ASA Statement on p-Values: Context, Process, and Purpose](https://amstat.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108?needAccess=true)

1. P-values can indicate how incompatible the data are with a specified statistical model.
2. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
4. Proper inference requires full reporting and transparency.
5. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

* [AMERICAN STATISTICAL ASSOCIATION RELEASES STATEMENT ON STATISTICAL SIGNIFICANCE AND P-VALUES
](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf)


### Public Data: UN Data and US Census

* UN Data: https://data.un.org
* Uited States Census Bureau: https://www.census.gov

#### Importing Data

1. Get the URL (uniform resource locator) - copy the link
  * `url_of_data <- "https://data.un.org/--long url--.csv"`
2. Download the file into the `destfile` in data folder:
  * `download.file(url = url_of_data, destfile = "data/un_pop.csv")`
3. Read the file:
  * `df_un_pop <- read_csv("data/un_pop.csv")`
  
### An Example: US Census Bureau

* [International Data Base (IDB)](https://www.census.gov/data-tools/demo/idb/#/country?YR_ANIM=2021)
* `idbzip.zip` is a large archive with three files; idb5yr.all, idbsingleyear.all and Readme.txt. The data of idbsingleyear.all is separated by | and contains over 3 million rows.
* Execute the following code only once. Then commnet these lines out with #.

#### Importance of Population Data of Each Country

* Population: Special meaning in statistics
  - In statistics, a population is a set of similar items or events which is of interest for some question or experiment.
* Basis of analysis
* Public Opinion Poll



### [OECD: About Who we are](https://www.oecd.org/about/)

**The Organisation for Economic Co-operation and Development** (OECD) is an international organisation that works to build better policies for better lives. Our goal is to shape policies that foster prosperity, equality, opportunity and well-being for all. 

#### Data

* http://www.oecd.org
* Data: https://data.oecd.org
* OECD.Stat: https://stats.oecd.org
  -  [OECD.Stat Web Browser User Guide, May 2013](https://stats.oecd.org/Content/themes/OECD/static/help/WBOS%20User%20Guide%20(EN).PDF)
    1. Search Box > Filter by type, Filter by topic, Sort results 
    2. Browse by Topic: Agriculture, Development, Economy, Education, Energy, Finance, Govetnment, Health, Innovation and Technology, Job, Society
    3. Browse by Country: choose from 43 countries
    4. Catalogue of OECD databases
    5. Featured Charts, Latest News, Statistical Resources 


### Learning Resources, V

* [R for Data Science, Part IV Model](https://r4ds.had.co.nz/model-intro.html#model-intro)
    
::: {.block}
#### RStudio Primers: See References in Moodle at the bottom

1. The Basics -- [r4ds: Explore, I](https://r4ds.had.co.nz/explore-intro.html#explore-intro)
2. Work with Data -- [r4ds: Wrangle, I](https://r4ds.had.co.nz/wrangle-intro.html#wrangle-intro)
3. Visualize Data -- [r4ds: Explore, II](https://r4ds.had.co.nz/explore-intro.html#explore-intro)
  - Exploratory Data Analysis, Bar Charts, Histograms
  - Boxplots and Counts, Scatterplots, Line Plots
  - Overplotting and Big Data, Customize Your Plots
4. Tidy Your Data -- [r4ds: Wrangle, II](https://r4ds.had.co.nz/wrangle-intro.html#wrangle-intro)
  - **Reshape Data, Separate and Unite Columns, Join Data Sets**
::: 


<!-- ### Assignment Six on Modeling and EDA (in Moodle) -->

<!-- * Choose a public data. Clearly state how you obtained the data.  -->
<!-- * Create an R Notebook (_file name_.nb.html, e.g. `a6_12345.nb.html`) of an EDA containing: -->

<!--   1. title, date, and author, i.e., Your Name -->
<!--   2. your motivation and/or objectives to analyse the data, and your questions -->
<!--   3. an explanation of the data and the variables -->
<!--   4. chunks containing the following: -->
<!--       - `library(tidyverse, modelr)`, -->
<!--       - visualization of the data with `ggplot()`  -->
<!--       - both a linear model (`summary(lm(y~x))`) and a chart created by `ggplot()` with fitted line. -->
<!--   5. explanation of the values given by the model and describe a prediction or a finding using the model. -->
<!--   6. your findings and/or questions -->

<!-- * Submit your R Notebook file to Moodle (The Sixth Assignment) by 2021-02-01 23:59:00 -->

<!-- --- -->

<!-- \vfill -->
<!-- \begin{center} -->
<!-- \LARGE The END of EDA Using tidyverse Packages -->
<!-- \end{center} -->

<!-- \vfill -->

### Learning Resources -- Summary

* **Books**
  - Textbook: [R for Data Science](https://r4ds.had.co.nz)
  - Books Online: [BOOKDOWN](https://bookdown.org)
* **Exercise Tools**
  - `{swirl}` [Learn R in R](https://swirlstats.com)
  - [R Studio Cloud Primers](https://rstudio.cloud/learn/primers)
* **Others**
  - [R Studio Education](https://education.rstudio.com/learn/)
  - [Cheat Sheet](https://www.rstudio.com/resources/cheatsheets/)


\vfill
> Data Science is an empirical study! Develop your skills and knowledge by experiences.
  
  
> Data Science is for Everyone!


## The Sixth Assignment (in Moodle) 

* Choose a public data. Clearly state how you obtained the data. Even if you are able to give the URL to download the data, explain the steps you reached and obtained the data. 
* Create an R Notebook (_file name_.nb.html, e.g. `a6_12345.nb.html`) of an EDA containing:
  1. title, date, and author, i.e., Your Name
  2. your motivation and/or objectives to analyze the data, and your questions
  3. an explanation of the data and the variables
  4. chunks containing the following:
      - `library(tidyverse, modelr)`,
      - visualization of the data with `ggplot()` 
      - both a linear model (`summary(lm(y~x))`) and a chart created by `ggplot()` with fitted line.
  5. explanation of the values given by the model and describe a prediction or a finding using the model.
  6. your findings and/or questions

* Submit your R Notebook file to Moodle (The Sixth Assignment) by 2021-02-01 23:59:00

### Setup and YAML

This R Notebook document uses the following YAML:

```
---
title: "Responses to the Sixth Assignment on Modeling and EDA"
author: "p000117x Hiroshi Suzuki"
date: '2022-02-07'
output: 
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float: yes
---
```

### Packages to load

https://tidyverse.tidyverse.org/

`library(tidyverse)` will load the core tidyverse packages:

* ggplot2, for data visualisation.
* dplyr, for data manipulation.
* tidyr, for data tidying.
* readr, for data import.
* purrr, for functional programming.
* tibble, for tibbles, a modern re-imagining of data frames.
* stringr, for strings.
* forcats, for factors.

Hence the following code chunk loads every core package above, and  `modelr`. Although `modelr` is a tidyverse package but it is not a core tidyverse package, you need to add `library(modelr)`.

```{r}
library(tidyverse)
library(modelr)
```

We will use `WDI` package for examples.

```{r}
library(WDI)
```


### Modeling and EDA Summary

Reference: 
* [r4ds:EDA](https://r4ds.had.co.nz/exploratory-data-analysis.html#exploratory-data-analysis)
* [R4DS: Model Basics](https://r4ds.had.co.nz/model-basics.html#model-basics)


1. Importing 
2. Glimpsing
3. Transforming
4. Visualising
5. Modeling

#### Exploration in Visualization

EDA is an iterative cycle. You:

1. Generate questions about your data.

2. Search for answers by visualising, transforming, and modelling your data. - _Methods in Data Science_

3. Use what you learn to refine your questions and/or generate new questions.

  
> “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox (1924.7.15-2022.1.18)

  
> “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey (1915.6.16-2000.7.26)

_Exploratory Data Analysis cannot be done by statistical routines or a list of routine questions._

#### Quotation Marks

* Reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Quotes.html

Three types of quotes are part of the syntax of R: single and double quotation marks and the backtick (or back quote, ‘⁠`⁠’).
Backslash is used to start an escape sequence inside character constants.

* Single and double quotes delimit character constants. They can be used interchangeably but double quotes are preferred (and character constants are printed using double quotes), so single quotes are normally only used to delimit character constants containing double quotes.
* 'single quotes can be used more-or-less interchangeably'
* "with double quotes to create character vectors"

```{r}
identical('"It\'s alive!", he screamed.',
          "\"It's alive!\", he screamed.") # same
```
* Backticks are used for non-standard variable names. (See make.names and ?Reserved for what counts as non-standard.)
  - _When you have a space, or other special characters, in the variable name, use backticks._
  
```{r}
`x y` <- 1:5  # variable
`x y`         # variable
```
```{r}
"x y" <- 1:5  # variable
`x y`         # variable
"x y"         # character vector
```

```{r}
d <- data.frame(`1st column` = rchisq(5, 2), check.names = FALSE)
d$`1st column`
```

### Examples and Comments

#### Preparations

##### Extra Packages

In the following we use `modelsummary` which Prof Kaizoji introduced at his Week 7 lecture.
You need to install it if you have not by the followiin in you console or from Tools menu on top.

See:  https://CRAN.R-project.org/package=modelsummary

```
install.packages("modelsummary")
```


```{r}
library(modelsummary) # Tool for writting tables of the regression results
```

### CLASS.xlsx

See Responses to Assignment Five.

#### Importing Excel Files

* CLASS.xlsx: - _copy the following link_
  - [The current classification by income in XLS format](https://databankfiles.worldbank.org/data/download/site-content/CLASS.xlsx) 
* readxl: https://readxl.tidyverse.org
* Help: `read_excel`, `read_xls`, `read_xlsx`

```{r cash = TRUE}
url_class <- "https://databankfiles.worldbank.org/data/download/site-content/CLASS.xlsx"
download.file(url = url_class, destfile = "data/CLASS.xlsx")
```
##### Countries

Let us look at the first sheet.

1. The column names are in the 5th row.
2. The country data starts from the 7th row.
3. Zimbabue is at the last row.

```{r}
library(readxl)
wb_countries_tmp <- read_excel("data/CLASS.xlsx", sheet = 1, skip = 0, n_max =219) 
wb_countries <- wb_countries_tmp %>% 
  select(country = Economy, iso3c = Code, region = Region, income = `Income group`, lending = "Lending category", other = "Other (EMU or HIPC)")
wb_countries
```

##### Regions

* readxl: https://readxl.tidyverse.org
* Help: `read_excel`, `read_xls`, `read_xlsx`

1. Regions start from the 221th row.
2. Regions end at the 266th row.

```{r}
wb_regions_tmp <- read_excel("data/CLASS.xlsx", sheet = 1, skip = 0, n_max =266) %>% 
  slice(-(1:220))
wb_regions <- wb_regions_tmp %>% 
  select(region = Economy, iso3c = Code) %>% drop_na()
wb_regions
```

Let us look at the second sheet.

```{r}
wb_groups_tmp <- read_excel("data/CLASS.xlsx", sheet = "Groups") # sheet = 3
wb_groups <- wb_groups_tmp %>% 
  select(gcode = GroupCode, group = GroupName, iso3c = CountryCode, country = CountryName)
```


### Population and GDP per Capita

#### Importing

```{r}
WDIsearch(string = "SP.POP.TOTL", field = "indicator")
```


```{r cash = TRUE}
df_pop_gdpcap <- WDI(
  country = "all",
  indicator = c(pop ="SP.POP.TOTL", gdpcap = "NY.GDP.PCAP.KD"),
  start = 1961,
  end = 2020
)
head(df_pop_gdpcap)
```

#### Visializing and Modeling

```{r}
df_pop_gdpcap %>% filter(country == "World") %>% 
  ggplot(aes(x = pop, y = gdpcap)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
```
```{r}
df_pop_gdpcap %>% filter(country == "World") -> df_pk
df_pk %>% lm(gdpcap ~ pop, .) %>% summary()
```

### Internet

#### Importing

```{r cash = TRUE}
df_un_internet <- read_csv("https://data.un.org/_Docs/SYB/CSV/SYB64_314_202110_Internet%20Usage.csv", skip = 1)

```
#### Glimpsing

We first overview the structure of the data briefly

```{r}
head(df_un_internet)
```

```{r}
glimpse(df_un_internet)
```
* Note that the first column name is surrounded by backticks. 

```{r}
df_un_internet %>% summary()
```


```{r}
df_un_internet %>% distinct(Source)
```
* All sources are 'International Telecommunication Union (ITU), Geneva, the ITU database, last accessed January 2021'.

```{r}
df_un_internet %>% distinct(Footnotes)
```

```{r}
df_un_internet %>% distinct(Series)
```
* There is only one series. 'Percentage of individuals using the internet' can be for the title.


```{r}
df_un_internet %>% group_by(Footnotes) %>% summarize(number_of_regions = n_distinct(`...2`)) %>%
  arrange(desc(number_of_regions))
```

* We need to see Footnotes carefully but in the beginning we do not examine.

```{r}
df_un_internet %>% distinct(Year)
```

```{r}
df_un_internet %>% group_by(Year) %>% summarize(number_of_regions = n_distinct(...2)) %>%
  arrange(desc(number_of_regions))
```

```{r}
df_un_internet %>% group_by(...2) %>% summarize(number_of_years = n_distinct(Year)) %>%
  arrange(number_of_years)
```

```{r}
df_un_internet %>% group_by(`Region/Country/Area`) %>% summarize(number_of_years = n_distinct(Year)) %>%
  arrange(number_of_years)
```

```{r}
df_un_internet %>% distinct(`Region/Country/Area`, ...2)
```

* Region/Country/Area and names in ...2 are in one to one correspondence. It seems that the first 21 of them are areas and data are aggregated and the rest of them are countries or regions.
* Since these 21 regions are overlapping, we need to study the definition. Let us choose 

```{r}
area_num_tmp <- c(15, 202, 21, 29, 420, 142, 150, 9)
```


#### Transforming 

```{r}
df_internet <- df_un_internet %>%
  select(num = `Region/Country/Area`, region = ...2, year = Year, value = Value) %>%
  mutate(year = as.integer(year))
head(df_internet)
```

```{r}
area_vector <- df_internet %>% distinct(region) %>% slice(1:21) %>% pull()
area_vector
```
#### Visualising

```{r}
df_internet %>% filter(region == "Total, all countries or areas") %>%
  ggplot(aes(x = year, y = value)) + geom_point() + geom_line()
```
The value, 'Percentage of individuals using the internet' is increasing and almost on a straight line. We will study linear regression later.

```{r}
df_internet %>% filter(num %in% area_num_tmp) %>%
  ggplot(aes(x = year, y = value, color = region)) + geom_line() + geom_point()
```


```{r}
df_internet %>% filter(region %in% area_vector) %>%
  ggplot(aes(x = year, y = value, color = region)) + geom_line()
```

```{r}
df_internet %>% filter(!(region %in% area_vector)) %>%
  ggplot(aes(x = factor(year), y = value)) + geom_boxplot() +
  labs(title = 'Percentage of individuals using the internet', x = 'year', y = 'percentage')
```
```{r}
df_internet %>% filter(!(region %in% area_vector)) %>% filter(year %in% c(2000, 2005, 2010, 2015, 2019)) %>%
  ggplot(aes(x = value, color = factor(year))) + geom_freqpoly() +
  labs(title = 'Percentage of individuals using the internet', x = 'percentage')
```

```{r}
df_internet %>% filter(!(region %in% area_vector)) %>% filter(year == 2019) %>%
  ggplot(aes(x = value)) + geom_histogram() +
  labs(title = 'Percentage of individuals using the internet', x = 'percentage')
```

* Only countries not in `area_vector` are used.
* Medians are increasing.
* There are lot of upper outliers in 2000 and 2005, and lower outliers in 2018 and 2020.

#### Modeling

```{r}
df_internet %>% filter(region == "Total, all countries or areas") %>%
  ggplot(aes(x = year, y = value)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
```

```{r}
internet_all <- df_internet %>% filter(region == "Total, all countries or areas") %>%
  lm(value ~ year, .) 
internet_all %>%  summary()
```

* Each year internet use is increasing by about 2.4 percents.

```{r}
df_internet %>% filter(num %in% area_num_tmp) %>%
  ggplot(aes(x = year, y = value)) + geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
internet_area <- df_internet %>% filter(num %in% area_num_tmp) %>%
  lm(value ~ year, .)
internet_area  %>% summary()
```

```{r}
df_internet %>% filter(!(region %in% area_vector)) %>%
  ggplot(aes(x = year, y = value)) + geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
internet_country <- df_internet %>% filter(!(region %in% area_vector)) %>%
  lm(value ~ year, .) 
internet_country %>% summary()
```
```{r}
internet_summary <- list(ALL = internet_all, By_Area = internet_area, By_Country = internet_country)
msummary(internet_summary, statistic = 'p.value')
```
What can you see? What are the difference of these three models? Which one is what you wanted?
All aggreagated growth, growth by area, or growth by country of the internet use?

### Bond Yields

#### Importing

* **Bonds and debentures** are long-term securities that give the holders the unconditional right to one or both of: (a) a fixed or contractually determined variable money income in the form of coupon payments, i.e. payment of interest is not dependent on earnings of the debtors, (b) a stated fixed sum as a repayment of principal on a specified date or dates when the security is redeemed.

* Data at United Nations: https://data.un.org/Data.aspx?q=bond+yeild&d=IFS&f=SeriesCode%3a61.
  - Reach the site by search string "bond yields"
  - Downloaded Data file in CSV: UNdata_Export_20220205_134850951.csv

```{r}
df_by <- read_csv("data/UNdata_Export_20220205_134850951.csv")
```

#### Glimpsing

```{r}
df_by
```

```{r}
df_by %>% glimpse()
```

```{r}
df_by %>% distinct(Description)
```

```{r}
df_by %>% group_by(Description) %>% summarize(num_of_regions = n_distinct(`Country or Area`)) %>%
  arrange(desc(num_of_regions))
```

```{r}
df_by %>% filter(Description == "GOVERNMENT BOND YIELD") %>%
  distinct(OID, `Country or Area`, Description)
```

```{r}
df_by %>% filter(Description != "GOVERNMENT BOND YIELD") %>%
  distinct(OID, `Country or Area`, Description)
```

```{r}
df_by %>% distinct(Magnitude)
```

```{r}
df_by %>% distinct(Year) %>% arrange(desc(Year))
```

```{r}
df_by %>% group_by(Year) %>% summarize(number_of_countries = n_distinct(`Country or Area`)) %>%
  arrange(desc(number_of_countries))
```

```{r}
df_by %>% distinct(OID)
```

```{r}
df_by %>% distinct(`Country or Area`)
```

```{r}
df_by %>% distinct(OID, `Country or Area`)
```

```{r}
df_by %>% group_by(`Country or Area`) %>% summarize(num_of_oid = n_distinct(OID)) %>%
  arrange(desc(num_of_oid))
```

* Seven countries have two OID and others have one OID.

#### Visualising

```{r}
df_by %>% filter(`Country or Area` == 'JAPAN') %>%
  ggplot(aes(x = Year, y = Value)) +
  geom_line()
```
```{r}
df_by %>% filter(`Country or Area` %in% c('FRANCE', 'GERMANY', 'GREECE', 'CHINA', 'INDIA', 'JAPAN', 'PAKISTAN')) %>%
  ggplot(aes(x = Year, y = Value, color = `Country or Area`)) +
  geom_line()
```

#### Modeling

```{r}
df_by %>% filter(`Country or Area` %in% c('JAPAN')) %>%
  ggplot(aes(x = Year, y = Value)) +
  geom_line() +
    geom_smooth(method = "lm", se = FALSE)
```
```{r}
df_by %>% filter(`Country or Area` %in% c('JAPAN')) %>%
  lm(Value ~ Year, .) %>% summary()
```



```{r}
df_by %>% filter(Description %in% c('GOVERNMENT BOND YIELD')) %>%
  ggplot(aes(x = Year, y = Value)) +
  scale_y_log10() +
  geom_point() + geom_smooth(method = "lm")
```
```{r}
df_by %>% filter(Description %in% c('GOVERNMENT BOND YIELD')) %>% filter(Year >=1980) %>% 
  ggplot(aes(x = Year, y = Value)) +
  scale_y_log10() +
  geom_point() + geom_smooth(method = "lm")
```
```{r}
df_by %>% filter(Description %in% c('GOVERNMENT BOND YIELD')) %>% filter(Year >=1980) %>% 
  lm(Value ~ Year, .) %>% summary()
```
* Very small R squared value, however we can see a downward trend as the coefficient of Year is about -0.3.

### Elderly Population

#### Importing

* OECD website: <https://data.oecd.org/>
  - Choose from "topic", "Society", and then "Elderly population"
    + URL: https://data.oecd.org/pop/elderly-population.htm
  - We can copy the URL of the download data
* The elderly population is defined as people aged 65 and over. 


Use of CLASS.xls

```{r cash = TRUE}
url_oecd_elderly <- "https://stats.oecd.org/sdmx-json/data/DP_LIVE/.ELDLYPOP.TOT.PC_POP.A/OECD?contentType=csv&detail=code&separator=comma&csv-lang=en&startPeriod=1970"
```

```{r cash = TRUE}
elderly_population <- read_csv(url_oecd_elderly)
```

#### Glimpsing

```{r}
elderly_population
```

```{r}
elderly_population %>% distinct(INDICATOR)
```

```{r}
elderly_population %>% distinct(SUBJECT)
```
```{r}
elderly_population %>% distinct(MEASURE)
```
```{r}
elderly_population %>% distinct(FREQUENCY)
```

```{r}
elderly_population %>% distinct(`Flag Codes`)
```
#### Transforming

```{r}
elderly_pop <- elderly_population %>% select(iso3c = LOCATION, year = TIME, value = Value)
```
```{r}
elderly_pop
```
```{r}
elderly_pop_ext <- elderly_pop %>% left_join(wb_countries, by = 'iso3c') 
elderly_pop_ext
```



```{r}
elderly_pop_ext %>% distinct(iso3c, country)
```

#### Visualising

The following render the same chart.

```{r}
elderly_pop_ext %>% filter(year == 2020, !is.na(region)) %>%
  ggplot(aes(region, fill = income)) + geom_bar() + coord_flip()
```

```{r}
elderly_pop_ext %>% filter(year == 2020, !is.na(region)) %>%
  ggplot(aes(y = region, fill = income)) + geom_bar(orientation = "y")
```


```{r}
elderly_pop_ext %>% filter(iso3c %in% c("AUS", "CAN", "DEU", "FRA", "GBR", "JPN", "KOR", "USA")) %>%
  ggplot(aes(x = year, y = value, color = country)) +
  geom_line()
```

```{r}
elderly_pop_ext %>% filter(income == "High income") %>%
  ggplot(aes(x = year, y = value, color = country)) +
  geom_line() + 
  labs(title = "High income")
```
```{r}
elderly_pop_ext %>% filter(income == "Upper middle income") %>%
  ggplot(aes(x = year, y = value, color = country)) +
  geom_line() + 
  labs(title = "Upper middle income")
```

```{r}
elderly_pop_ext %>% filter(income == "Lower middle income") %>%
  ggplot(aes(x = year, y = value, color = country)) +
  geom_line() + 
  labs(title = "Lower middle income")
```
#### Modeling

```{r}
elderly_pop_all <- elderly_pop_ext %>% 
  lm(value ~ year, .)
elderly_pop_all %>% summary()
```

```{r}
elderly_pop_high <- elderly_pop_ext %>% filter(income == "High income") %>%
  lm(value ~ year, .)
elderly_pop_high %>% summary()
```
```{r}
elderly_pop_upper_middle <- elderly_pop_ext %>% filter(income == "Upper middle income") %>%
  lm(value ~ year, .)
elderly_pop_upper_middle %>% summary()
```

```{r}
elderly_pop_lower_middle <- elderly_pop_ext %>% filter(income == "Lower middle income") %>%
  lm(value ~ year, .)
elderly_pop_lower_middle %>% summary()
```
```{r}
elderly_pop_summary <- list(ALL = elderly_pop_all,High = elderly_pop_high, Upper_middle = elderly_pop_upper_middle, Loer_middle = elderly_pop_lower_middle)
msummary(elderly_pop_summary, statistic = 'p.value')
```

* All data show the increase of the population of elderly people.
* There are differences in income level. Can you sess? Look at year, the slope.
* Can we state that they are significangly different? It is a statistical problem.



### Education

#### Importing

 1. Go to UN data website: https://data.un.org/
 2. At the bottom in popular statistical tables, scroll to education
 3. download from "education at the primary, secondary and tertiary level"
 

```{r}
edu_tmp <- read_csv("data/edu.csv", skip = 1, n_max =6942) %>% slice(-1)
edu_tmp
```

#### Transforming

```{r}
edu_data <- edu_tmp %>% select(`Region/Country/Area`:Source)
edu_data
```

```{r}
colnames(edu_data)
```

```{r}
edu_tidy <- edu_data %>%
  select(country = `...2`, year = Year, series = Series, value = Value)
edu_tidy
```

```{r}
edu_tidy %>% distinct(series)
```

```{r}
edu_tidy %>% group_by(year) %>% summarize(n = n_distinct(country)) %>% arrange(desc(n))
```

#### Modeling

From the correlation, we can see R squared values.

```{r}
edu_tidy %>% filter(year == 2019) %>% 
  pivot_wider(names_from = series, values_from = value) %>% select(primary = 3, secondary = 6, tertiary = 9) %>% drop_na %>% 
  cor()
```

```{r}
edu_tidy %>% filter(year == 2019) %>% 
  pivot_wider(names_from = series, values_from = value) %>% select(primary = 3, p_male = 4, p_female = 5, secondary = 6, s_mele = 7, s_female = 8, tertiary = 9, t_male = 10, t_female = 11) %>% drop_na %>% 
  cor()
```

```{r}
edu_tidy %>% filter(year == 2019) %>% 
  pivot_wider(names_from = series, values_from = value) %>% 
  select(primary = 3, secondary = 6, tertiary = 9) %>% 
  drop_na %>%
  ggplot(aes(x = secondary, y = tertiary)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
```

```{r}
edu_tidy %>% filter(year == 2019) %>% 
  pivot_wider(names_from = series, values_from = value) %>% 
  select(primary = 3, secondary = 6, tertiary = 9) %>% 
  drop_na %>%
  filter(secondary < 100000) %>% 
  ggplot(aes(x = secondary, y = tertiary)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
```


```{r}
edu_tidy %>% filter(year == 2019) %>% 
  pivot_wider(names_from = series, values_from = value) %>% 
  select(primary = 3, secondary = 6, tertiary = 9) %>% 
  drop_na %>%
  lm(tertiary ~ secondary, .) %>% summary()
```



