# Appendix C Modeling {#model}

* Modeling using `tidyverse`

### EDA (A diagram from R4DS by H.W. and G.G.){-}

![EDA from r4ds](./data/data-science.png)

### Contents of EDA5 {-}

> Data Science is empirical!?

**empirical**: based on, concerned with, or verifiable by observation or experience rather than theory or pure logic  

**Part I: Data Modeling**

* Exploration in Visualization

* Modeling

  -  Scientific: Why? Prediction! - Do you support this?
  -  Evidence based! - What does it mean? 
  -  What is regression, and why regression? 
  -  Linear Regression, ggplot2
  -  Predictions and Residues

**Part II: Examples and Practicum**



## Exploration in Visualization

Reference: [r4ds:EDA](https://r4ds.had.co.nz/exploratory-data-analysis.html#exploratory-data-analysis)

### Exploration in Visualization, I

EDA is an iterative cycle. You:

1. Generate questions about your data.

2. Search for answers by visualising, transforming, and modelling your data. - _Methods in Data Science_

3. Use what you learn to refine your questions and/or generate new questions.

  
> “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox (1924.7.15-2022.1.18)

[Obituary:](https://rss.org.uk/news-publication/news-publications/2022/general-news/sir-david-cox-1924-2022/) Sir David Cox, 1924-2022, Royal Statistical Society

  
> “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey (1915.6.16-2000.7.26)

_Exploratory Data Analysis cannot be done by statistical routines or a list of routine questions._

### Exploration in Visualization, II 

**The very basic questions:**

* What type of variation occurs within my variables? --_one variable_

* What type of covariation occurs between my variables? - _two or more variables_

  
**Typical values**

* Which values are the most common? Why?
  - median, mean, mode, etc.

* Which values are rare? Why? Does that match your expectations?
  - outlier, exceptional

* Can you see any unusual patterns? What might explain them?

### Exploration in Visualization, III 

**Clusters and Groups**

* How are the observations within each cluster similar to each other?

* How are the observations in separate clusters different from each other?

* How can you explain or describe the clusters?

* Why might the appearance of clusters be misleading?  

  
**Outliers and Unusual Values**

Sometimes outliers are data entry errors; other times outliers suggest important new science. 

## Exploration in Visualization, IV 

**Patterns and models** 

* Could this pattern be due to coincidence (i.e. random chance)?

* How can you describe the relationship implied by the pattern?

* How strong is the relationship implied by the pattern?

* What other variables might affect the relationship?

* Does the relationship change if you look at individual subgroups of the data?

  

## Exploration and Data Modeling

**Model is a simple summary of data**

> **Goal**: A simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in). 

  1. “predictive” models: supervised
  2. “data discovery” models: unsupervised
	

### Hypothesis generation vs. hypothesis confirmation

1. Each observation can either be used for exploration or confirmation, not both.

2. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration.

If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis:

1. 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it.

2. 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process.

3. 20% is held back for a test set. You can only use this data ONCE, to test your final model.

### Model Basics

Reference:  [R4DS: Model Basics](https://r4ds.had.co.nz/model-basics.html#model-basics)
  

There are two parts to a model:

1. First, you define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like $y = a_1 * x + a_2$ or $y = a_1 * x ^ {a_2}$. Here, $x$ and $y$ are known variables from your data, and $a_1$ and $a_2$ are parameters that can vary to capture different patterns.

2. Next, you generate a fitted model by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like $y = 3 * x + 7$ or $y = 9 * x ^ 2$.

It’s important to understand that **a fitted model is just the closest model from a family of models**. That implies that you have the “best” model (according to some criteria); it doesn’t imply that you have a good model and it certainly doesn’t imply that the model is “true”. 

> All models are wrong, but some are useful.   \hfill - George E.P. Box (1919-2013)

Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an “ideal” gas via a constant R is not exactly true for any real gas, but it frequently provides a **useful approximation** and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.

For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “**Is the model illuminating and useful?**”.

  
> The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.

#### Regression Analysis ([wikipedia](https://en.wikipedia.org/wiki/Regression_analysis))

In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable ('outcome variable') and one or more independent variables ('predictors', 'covariates', or 'features'). The most common form of regression analysis is linear regression, in which a researcher finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. 

Two purposes of regression analysis: First, regression analysis is widely used for **prediction and forecasting**. Second, regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. 

#### History of Regression Analysis  ([wikipedia](https://en.wikipedia.org/wiki/Regression_analysis))


The earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss–Markov theorem.

The term "regression" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to **regress** down towards a normal average (a phenomenon also known as regression toward the mean). For Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context.


#### Galton's Example

We use `HistData` Package. See https://CRAN.R-project.org/package=HistData.

```{r}
library(tidyverse)
library(modelr)
library(datasets)
library(HistData)
```

**Galton's data on the heights of parents and their children, by child**

* **Description:**
This data set lists the individual observations for 934 children in 205 families on which Galton (1886) based his cross-tabulation.

In addition to the question of the relation between heights of parents and their offspring, for which this data is mainly famous, Galton had another purpose which the data in this form allows to address: Does marriage selection indicate a relationship between the heights of husbands and wives, a topic he called assortative mating? Keen [p. 297-298](2010) provides a brief discussion of this topic.

* See Help: GaltonFamilies


```{r}
gf <- as_tibble(GaltonFamilies)
gf
```



```{r}
gf %>% filter(gender == "male") %>%
  ggplot() +
  geom_point(aes(father, childHeight)) +
  labs(title = "GaltonFamilies", x = "father's height", y = "son's height")
```

```{r}
gf %>% filter(gender == "female") %>%
  ggplot() +
  geom_point(aes(mother, childHeight)) +
  labs(title = "GaltonFamilies", x = "mother's height", y = "daughter's height")
```
  
>"The heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean)." 

```{r}
gf %>% filter(gender == "male") %>%
  ggplot(aes(father, childHeight)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "GaltonFamilies", x = "father's height", y = "son's height")
```

```{r}
gf %>% filter(gender == "female") %>%
  ggplot(aes(mother, childHeight)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "GaltonFamilies", x = "mother's height", y = "daughter's height")
```

```{r}
gf %>% filter(gender == "male") %>% 
  lm(childHeight ~ father, data = .) %>% summary()
```
```{r}
gf %>% filter(gender == "female") %>%
  lm(childHeight ~ mother, data = .) %>% summary()
```
* **midparentHeight:**
mid-parent height, calculated as (father + 1.08*mother)/2

```{r}
gf %>% filter(gender == "male") %>% 
  lm(childHeight ~ midparentHeight, data = .) %>% summary()
```

#### Summary of the Data

```{r}
gf %>% summary()
```

The following information can be found in summary above.

```{r}
gf %>% summarize(n_father = sum(!is.na(father)), 
                 n_mother = sum(!is.na(mother)), 
                 n_male = sum(gender == "male"),
                 n_female = sum(gender == "female"))
```
#### What type of variation occurs within my variables?

```{r}
gf %>% select(childHeight) %>% summary()
```


```{r}
gf %>% ggplot() +
  geom_boxplot(aes(x = childHeight))
```

* [`geom_boxplot()`](https://ggplot2.tidyverse.org/reference/geom_boxplot.html)


```{r}
gf %>% ggplot() +
  geom_histogram(aes(x = childHeight))
```

```{r}
gf %>% select(gender, childHeight) %>%
  group_by(gender) %>%
  summarize(q1 = quantile(childHeight, 0.25), med = median(childHeight), q3 = quantile(childHeight, 0.75), left = q1 - 1.5*(q3-q1), right = q3 + 1.5*(q3-q1))
```


```{r}
gf %>% ggplot() +
  geom_boxplot(aes(x = childHeight, y = gender))
```

```{r}
gf %>% ggplot() + 
  geom_histogram(aes(x = childHeight, fill = gender), alpha = 0.7)
```
```{r}
gf %>% ggplot() + 
  geom_histogram(aes(x = childHeight, fill = gender)) +
  facet_wrap(vars(gender))
```

```{r}
mm <- gf %>% filter(gender == "male") %>% 
  summarize(m = sum(!is.na(childHeight)), ave = mean(childHeight), se = sd(childHeight))
mm
```
```{r}
gf %>% filter(gender == "male") %>%
  ggplot(aes(x = childHeight)) + 
  geom_histogram() +
  geom_vline(xintercept = mm$ave, color = "red") +
  geom_vline(xintercept = mm$ave - mm$se * 1.96, color = "blue") + 
  geom_vline(xintercept = mm$ave + mm$se * 1.96, color = "blue")
```

```{r}
gf %>% filter(gender == "male") %>% 
  filter(childHeight >= mm$ave - mm$se * 1.96) %>%
  filter(childHeight <= mm$ave + mm$se * 1.96) %>%
  summarize(n = sum(!is.na(childHeight)), ratio = n/mm$m)
```
```{r, echo = FALSE}
df <- 48 # degree of freedom
funcShaded <- function(x) {
    y <- dt(x, df)
    y[x < -1.96 | x > 1.96] <- NA
    return(y)
}
funcShaded2 <- function(x) {
    y <- dt(x, df)
    y[x > -1.92 & x < 1.92] <- NA
    return(y)
}
ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = function(x){dt(x, df)}, color = "black", size = 1) + 
  stat_function(fun = funcShaded, geom = "area", fill = "blue", alpha = 0.3) +
  stat_function(fun = funcShaded2, geom = "area", fill = "red", alpha = 0.3) +
  labs(title = "95% Interval: p(-1.96 < x < 1.96)") +
  annotate("text", label = "0.95", x = 0, y = 0.2, size = 6) +
  annotate("text", label = "95%", x = 0, y = 0.1, size = 8) +
  annotate("text", label = "0.025", x = 2.7, y = 0.06, size = 6) +
  annotate("text", label = "0.025", x = -2.7, y = 0.06, size = 6)
```

```{r}
gf %>% filter(gender == "male") %>% 
  ggplot(aes(sample = childHeight)) +  
  stat_qq() +
  stat_qq_line(col = "blue")
```

```{r}
gf %>% filter(gender == "female") %>% 
  ggplot(aes(sample = childHeight)) +  
  stat_qq() +
  stat_qq_line(col = "blue")
```

```{r}
gf %>% 
  ggplot(aes(sample = childHeight)) +  
  stat_qq() +
  stat_qq_line(col = "red")
```

```{r}
gf %>% ggplot() + 
  geom_freqpoly(aes(x = childHeight, color = gender))
```

```{r}
gf %>% ggplot() +
  geom_histogram(aes(x = father), fill = "blue", alpha = 0.5) + 
  geom_histogram(aes(x = mother), fill = "red", alpha = 0.5)
```


```{r}
gf %>% ggplot() +
  geom_freqpoly(aes(x = father), color = "blue") + 
  geom_freqpoly(aes(x = mother), color = "red")
```



```{r}
gf %>% ggplot() +
  geom_boxplot(aes(x = gender, y = childHeight))
```

```{r}
gf %>% select(father, mother) %>%
  pivot_longer(cols = everything(), names_to = "parents", values_to = "height") %>%
  ggplot() +
  geom_boxplot(aes(x = parents, y = height))
```
##### R commands on statistical measurements

**One Variable**

* mean: average, i.e., the sum of values divided by the number of values.
  - `mean()` or `mean(x, na.rm = TRUE)`
* median: middle value, i.e., the value separating the higher half from the lower half of a data sample.
  - `median()` or `median(x, na.rm = TRUE)`
* quantile: `quantile()` or `quantile(x, na.rm = TRUE)`
* variance: spread from the mean i.e., the expectation of the squared standard deviation of a variable from its mean, or how much a set of numbers are spread out from the mean.
  - `var()` or `var(x, na.rm = TRUE)`
* standard deviation: a measure of spread from the mean, i.e., a measure that is used to quantify the amount of variation or dispersion of a set of data value. The standard deviation is the square root of the variance.
  - `sd()`
  
  
**More Than One variable**

* covariance: a measure indicating the extent to which two random variables change in tandem (either positively or negatively). It’s unit is that of the variable. A large covariance can mean a strong relationship between variables.
  - `cov()`
* correlation:  a statistical measure that indicates how strongly two variables are related. It’s a scaled version of covariance. It’s value always lies between -1 and +1.
  - `cor()`, cor(x, y, use = "everything")
    + A resulting value will be NA whenever one of its contributing observations is NA.
  - `cor(x, y, use = "pairwise.complete.obs")
    + It uses all complete pairs of observations on those variables.

* summary()

#### What type of covariation occurs between my variables?

```{r}
gf %>% ggplot() +
  geom_point(aes(x = father, y = childHeight, color = gender))
```

```{r}
gf %>% ggplot() +
  geom_point(aes(x = father, y = childHeight, color = gender)) +
  facet_wrap(vars(gender))
```

```{r}
gf %>% select(father, mother, gender, childHeight) %>%
  group_by(gender) %>% 
  summarize("cor_w/father" = cor(father, childHeight), "sq_w/father" = cor(father, childHeight)^2, "cor_w/mother" = cor(mother, childHeight), "sq_w/mother" = cor(mother, childHeight)^2)
```

* Let $x = c(x_1, x_2, \ldots, x_n)$ be the independent variable, e.g., the height of fathers
* Let $y = c(y_1, y_2, \ldots, y_n)$ be the dependent variable, i.e., height of children
* Let $\mbox{pred} = c(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_n)$ be the predicted values by linear regression.

$$
\begin{aligned}
\mbox{unbiased covariance: } cov(x,y) &= \frac{\sum_{i=1}^n(x_i - mean(x))(y_i - mean(y))}{n-1}\\
\mbox{unviased variance: } var(x) &= cov(x,x) \\
\mbox{correlation: } cor(x.y) &= \frac{cov(x,y)}{\sqrt{var(x)var(y)}}\\
\mbox{slope of the regression line}  &= \frac{cov(x,y)}{var(x)} = \frac{cor(x,y)\sqrt{var(y)}}{\sqrt{var(x)}}\\
\mbox{total sum of squares} &= SS_{tot} = \sum_{i=1}^n((y_i-mean(y))^2)\\
\mbox{residual sum of squares} &= SS_{res} = \sum_{i=1}^n((y_i-\hat{y}_i)^2)\\
\mbox{R squared} = R^2 & = 1 - \frac{SS_{res}}{SS_{tot}} = cor(x,y)^2
\end{aligned}
$$

```{r}
gf %>% ggplot(aes(father, childHeight, color = gender)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) 
```

```{r}
gf %>% ggplot(aes(father, childHeight, group = gender)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(vars(gender))
```

```{r}
mod_mf <- gf %>% filter(gender == "male") %>% lm(childHeight ~ father, .) 
summary(mod_mf)
gf %>% filter(gender == "male") %>% 
  add_predictions(mod_mf) %>% 
  add_residuals(mod_mf)
```
```{r}
mod_ff <- gf %>% filter(gender == "female") %>% lm(childHeight ~ father, .) 
summary(mod_ff)
gf %>% filter(gender == "female") %>% 
  add_predictions(mod_ff) %>% 
  add_residuals(mod_ff)
```

##### Basic Facts

1. \(-1 \leq cor(x,y) \leq 1\): positive (or negative) correlation
2. If \(cor(x,y)\) is positive [resp. negative], the slope of the regression line is positive [resp. negative].
3. If the possibility of the slope of the regression line being zero is very low, $p$-value of the slope becomes very small.
4. R squared is the square of \(cor(x,y)\) which is between 0 and 1.
5. If the regression line fits well, R squared value is close to 1.

What do you want to observe from this dataset?

Reference: [Introduction to Data Science by Rafael A. Irizarry, Chapter 18 Regression](https://rafalab.github.io/dsbook/regression.html#regression) includes a Galton's example.


#### Visual Illustration

##### Example 1. Strong positive correlation

```{r}
set.seed(12345)
dt1 <- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, 0.8, 0.8, 1), 2,2))) 
colnames(dt1) <- c("V1", "V2")
dt1 %>% ggplot() + geom_point(aes(x = V1, y = V2))
```
```{r}
dt1 %>% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2))
```
```{r}
dt1 %>% ggplot(aes(V1, V2)) +
  geom_point() +
  geom_vline(xintercept = mean(dt1$V1), color = "red") +
  geom_hline(yintercept = mean(dt1$V2), color = "red") +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
dt1 %>% lm(V2~V1, .) %>% summary()
```

##### Example 2. Weak positive correlation

```{r}
set.seed(12345)
dt2 <- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, 0.2, 0.2, 1), 2,2))) 
colnames(dt2) <- c("V1", "V2")
dt2 %>% ggplot() + geom_point(aes(x = V1, y = V2))
```
```{r}
dt2 %>% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2))
```
```{r}
dt2 %>% ggplot(aes(V1, V2)) +
  geom_point() +
  geom_vline(xintercept = mean(dt2$V1), color = "red") +
  geom_hline(yintercept = mean(dt2$V2), color = "red") +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
dt2 %>% lm(V2~V1, .) %>% summary()
```


##### Example 3. Strong negative correlation

```{r}
set.seed(12345)
dt3 <- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, -0.8, -0.8, 1), 2,2))) 
colnames(dt3) <- c("V1", "V2")
dt3 %>% ggplot() + geom_point(aes(x = V1, y = V2))
```
```{r}
dt3 %>% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2))
```
```{r}
dt3 %>% ggplot(aes(V1, V2)) +
  geom_point() +
  geom_vline(xintercept = mean(dt1$V1), color = "red") +
  geom_hline(yintercept = mean(dt1$V2), color = "red") +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
dt3 %>% lm(V2~V1, .) %>% summary()
```


##### Example 4. Weak negative correlation

```{r}
set.seed(12345)
dt4 <- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, -0.2, -0.2, 1), 2,2))) 
colnames(dt4) <- c("V1", "V2")
dt4 %>% ggplot() + geom_point(aes(x = V1, y = V2))
```
```{r}
dt4 %>% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2))
```
```{r}
dt4 %>% ggplot(aes(V1, V2)) +
  geom_point() +
  geom_vline(xintercept = mean(dt1$V1), color = "red") +
  geom_hline(yintercept = mean(dt1$V2), color = "red") +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
dt4 %>% lm(V2~V1, .) %>% summary()
```


## Examples

### `cars`

```{r}
data(cars)
```


```{r}
plot(cars)
```

```{r}
plot(cars) # cars: Speed and Stopping Distances of Cars
abline(lm(cars$dist~cars$speed))
```

```{r}
summary(lm(cars$dist~cars$speed))
```

#### Model Analysis with `tidyverse`: `modelr`

* Tidymodels: https://www.tidymodels.org
* `modelr`: https://modelr.tidyverse.org
  - [Compute model quality for a given dataset](https://modelr.tidyverse.org/reference/model-quality.html)

```{r}
t_cars <- as_tibble(cars)
```

```{r}
ggplot(t_cars) +
  geom_point(aes(speed, dist))
```

```{r}
mod <- lm(dist ~ speed, data = t_cars)
mod
```


```{r}
summary(mod)
```

Note that `cor(speed, dist)` = `r cor(cars$speed, cars$dist)`.


```{r}
ggplot(t_cars, aes(speed, dist)) + geom_point() +
  geom_abline(slope = coef(mod)[[2]], intercept = coef(mod)[[1]])
```

```{r}
ggplot(t_cars, aes(speed, dist)) + geom_jitter() +
  geom_smooth(method = "lm", se = FALSE)
```

#### Predictions $\hat{y} = a_1 + a_2x$ and Residuals $y - \hat{y}$

```{r}
mod_table <- t_cars %>% add_predictions(mod) %>% add_residuals(mod, var = "resid")
mod_table
```

```{r}
summary(mod_table)
```

```{r}
mod_table %>% 
  ggplot() + geom_jitter(aes(speed, resid)) + 
  geom_hline(yintercept = 0)
```

* `geom_jitter()`

```{r}
t_cars %>% group_by(speed, dist) %>% 
  summarize(count = n()) %>% arrange(desc(count))
```


### iris

```{r}
data(iris)
```


```{r}
t_iris <- as_tibble(iris)
t_iris
```



#### Linear Model: Sepal.Width ~ Sepal.Length

```{r}
lm(Sepal.Width ~ Sepal.Length, data = t_iris)
```

#### Linear Model: Petal.Width ~ Petal.Length

```{r}
lm(Petal.Width ~ Petal.Length, data = t_iris)
```

#### Correlation

```{r}
cor(t_iris[1:4])
```

#### `filter(Species == "setosa")`

```{r echo = FALSE, message = FALSE}
t_iris %>% filter(Species == "setosa") %>%
  ggplot(aes(x = Petal.Length, y = Petal.Width)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Setosa Only")
```

#### `aes(Sepal.Length, Sepal.Width, color = Species)`

```{r}
ggplot(t_iris, aes(Sepal.Length, Sepal.Width, color = Species)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) 
```


### Model Analysis Warnings

#### Linear Regression: `aes(Sepal.Length, Sepal.Width, color = Species)`

```{r message = FALSE, echo = FALSE}
ggplot(t_iris, aes(Sepal.Length, Sepal.Width, color = Species)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) 
```

#### `aes(Sepal.Length, Sepal.Width, color = Species)`

```{r message = FALSE, echo = FALSE}
ggplot(t_iris, aes(x = Sepal.Length, y = Sepal.Width)) + 
  geom_point(aes(color = Species)) + 
  geom_smooth(aes(color = Species), formula = y ~ x, method = "lm", se = FALSE) +
  geom_smooth(formula =  y ~ x, method = "lm", se = FALSE, color = "black", linetype = "twodash", size = 1.5)
```

#### Correlation and Covariance

```{r}
cor(t_iris[1:4])
```

```{r}
cov(t_iris[1:4])
```


### Regression Analyais: Summary

1. R-Squared:	Higher the better (> 0.70)
   - How well the prediction fit to the data. $0 \leq R2 \leq 1$.
   - For linear regression between two variables `x` and `y`, R-squared is the square of `cor(x,y)`.
   - R-Squared can be measured on any prediction model.
2. t-statistic:	The absolute value should be greater 1.96, and for p-value be less than 0.05
   - The model and the choice of variable(s) are suitable or not.
   - p-value appears in Hypothesis testing (A/B test, sensitivity - specificity, etc.)
   

#### [The ASA Statement on p-Values: Context, Process, and Purpose](https://amstat.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108?needAccess=true)

1. P-values can indicate how incompatible the data are with a specified statistical model.
2. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
4. Proper inference requires full reporting and transparency.
5. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

* [AMERICAN STATISTICAL ASSOCIATION RELEASES STATEMENT ON STATISTICAL SIGNIFICANCE AND P-VALUES
](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf)

#### Linear Regression Quick Reference

* Textbook: R for Data Science
  - Introduction to Model: https://r4ds.had.co.nz/model-intro.html#model-intro
  - Model Basics: https://r4ds.had.co.nz/model-basics.html#model-basics

* Introduction to Data Science by Rafael A. Irizarry, Chapters 18, 19: 
  - Regression: https://rafalab.github.io/dsbook/regression.html#regression
  - Linear Models: https://rafalab.github.io/dsbook/linear-models.html#linear-models
  
* DataCamp: 
  - https://www.datacamp.com/community/tutorials/linear-regression-R
    1. What is a linear regression?
    2.  Creating a linear regression in R.
    3. Learn the concepts of coefficients and residuals.
    4. How to test if your linear model has a good fit?
    5. Detecting influential points.
  - DataCamp Top: https://www.datacamp.com - to be introduced

* r-statistics.co by Selva Prabhakaran: 
  - http://r-statistics.co/Linear-Regression.html
    + The aim of linear regression is to model a continuous variable Y as a mathematical function of one or more X variable(s), so that we can use this regression model to predict the Y when only the X is known. This mathematical equation can be generalized as follows:
    + $Y = \beta_1 + \beta_2 X + \varepsilon$
  - r-statistics.co Top: http://r-statistics.co
    + An educational resource for those seeking knowledge related to machine learning and statistical computing in R.
